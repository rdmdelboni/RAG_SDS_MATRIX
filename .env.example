# RAG_SDS_MATRIX Configuration
# Copy this file to .env and adjust as needed

# === Ollama Configuration ===
OLLAMA_BASE_URL=http://localhost:11434

# Models (adjust based on your installed models)
OLLAMA_EXTRACTION_MODEL=qwen2.5:7b-instruct-q4_K_M
OLLAMA_CHAT_MODEL=llama3.1:8b
OLLAMA_EMBEDDING_MODEL=qwen3-embedding:4b
OLLAMA_OCR_MODEL=deepseek-ocr:latest

# === Processing Settings ===
MAX_WORKERS=8
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
MAX_FILE_SIZE_MB=50

# === LLM Parameters ===
LLM_TEMPERATURE=0.1
LLM_MAX_TOKENS=2000
LLM_TIMEOUT=120

# === Paths (optional - defaults to ./data/) ===
# DATA_DIR=/custom/path/to/data
# CHROMA_DB_PATH=/custom/path/to/chroma
# DUCKDB_PATH=/custom/path/to/duckdb

# === Logging ===
LOG_LEVEL=INFO

# === UI Settings ===
UI_LANGUAGE=pt  # pt or en
UI_THEME=dark   # dark or light

# === Optional: Bright Data Automated Crawl ===
# IMPORTANT: Store real API keys in .env.local (not tracked by git)
# BRIGHTDATA_API_KEY=your_brightdata_key
# BRIGHTDATA_DATASET_ID=gd_lr##########   # replace with your dataset id
# DATASET_STORAGE_FOLDER=./data/datasets/
# SNAPSHOT_STORAGE_FILE=./data/brightdata_snapshot.txt

# === Optional: Google Custom Search (used by knowledge ingestion) ===
# IMPORTANT: Store real API keys in .env.local (not tracked by git)
# GOOGLE_API_KEY=your_google_api_key
# GOOGLE_CSE_ID=your_custom_search_engine_id

# === Optional: Craw4AI Integration ===
# CRAW4AI_COMMAND=crawl4ai run --mode {mode} --input {input_file} --output {output_file}
# CRAW4AI_OUTPUT_DIR=./data/craw4ai
